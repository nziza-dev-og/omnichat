
"use client";

import React, { useState, useCallback, useRef, useEffect } from "react";
import MessageList from "./MessageList";
import ChatInput from "./ChatInput";
import { useToast } from "@/hooks/use-toast";
import { handleContext } from "@/ai/flows/context-handling";
import type { HandleContextInput, HandleContextOutput } from "@/ai/flows/context-handling";
import { generateCode } from "@/ai/flows/code-generation";
import type { GenerateCodeInput, GenerateCodeOutput } from "@/ai/flows/code-generation";
import { generateImage } from "@/ai/flows/image-generation";
import type { GenerateImageInput, GenerateImageOutput } from "@/ai/flows/image-generation";
import { textToSpeech, type TextToSpeechInput, type TextToSpeechOutput } from "@/ai/flows/text-to-speech-flow";
import { availableModels } from "./ModelSelector";
import type { PlaygroundContentType } from "./PlaygroundArea";
import { Howl } from 'howler';

export interface Message {
  id: string;
  role: "user" | "assistant";
  content: string;
  modelUsed?: string;
  timestamp: Date;
  audioDataUri?: string;
}

interface ChatInterfaceProps {
  onContentGenerated: (content: string, type: PlaygroundContentType, title: string, language: string | null) => void;
}

export default function ChatInterface({ onContentGenerated }: ChatInterfaceProps) {
  const [messages, setMessages] = useState<Message[]>([
    {
      id: "initial-bot-message",
      role: "assistant",
      content: "Hello! I am OmniAssist. How can I help you with your coding tasks, image ideas, or other questions today?",
      timestamp: new Date(),
      modelUsed: "OmniAssist (Together AI)"
    }
  ]);
  const [isLoading, setIsLoading] = useState(false);
  const [conversationHistory, setConversationHistory] = useState<string>("");
  const { toast } = useToast();
  const currentAudio = useRef<Howl | null>(null);

  const playAudioFromDataUri = useCallback((dataUri: string) => {
    if (currentAudio.current) {
      currentAudio.current.stop();
      currentAudio.current.unload();
    }
    if (!dataUri) return;

    currentAudio.current = new Howl({
      src: [dataUri],
      format: ['mp3'], // Explicitly state format, though Howler often detects
      html5: true, // Use HTML5 Audio where available
      onplayerror: (id, err) => {
        console.error('Howler play error:', id, err);
        toast({
          title: "Audio Playback Error",
          description: "Could not play audio.",
          variant: "destructive",
        });
      },
      onloaderror: (id, err) => {
        console.error('Howler load error:', id, err);
        toast({
          title: "Audio Load Error",
          description: "Could not load audio for playback.",
          variant: "destructive",
        });
      }
    });
    currentAudio.current.play();
  }, [toast]);

  const addMessage = (message: Omit<Message, "id" | "timestamp"> & { id?: string }) => {
    setMessages((prevMessages) => [
      ...prevMessages,
      { ...message, id: message.id || crypto.randomUUID(), timestamp: new Date() },
    ]);
  };

  const handleNewChat = useCallback(() => {
    setMessages([
      {
        id: "initial-bot-message-reset",
        role: "assistant",
        content: "Hello! I am OmniAssist. How can I help you with your coding tasks, image ideas, or other questions today?",
        timestamp: new Date(),
        modelUsed: "OmniAssist (Together AI)"
      }
    ]);
    setConversationHistory("");
    if (typeof onContentGenerated === 'function') {
      onContentGenerated(
        "Welcome to OmniAssist! Code generated by OmniAssist will appear in the playground if it's selected.",
        'welcome', 
        "Code Playground",
        null // No language for welcome message
      );
    }
    if (currentAudio.current) {
      currentAudio.current.stop();
    }
    toast({ title: "New Chat Started", description: "Previous conversation has been cleared." });
  }, [onContentGenerated, toast]);

  const handleSendMessage = useCallback(
    async (userInput: string, selectedModelValue: string) => {
      const userMessageId = crypto.randomUUID();
      addMessage({ id: userMessageId, role: "user", content: userInput });
      setIsLoading(true);

      const currentModelInfo = availableModels.find(m => m.value === selectedModelValue);
      const modelDisplayName = currentModelInfo?.label || selectedModelValue;
      const playgroundTitleBase = userInput.length > 30 ? userInput.substring(0, 27) + "..." : userInput;
      
      let assistantResponseMessageContent = ""; 
      let textForTTS = ""; 
      let generatedAudioDataUri: string | undefined = undefined;
      let assistantMessageId = crypto.randomUUID();


      try {
        if (selectedModelValue === "together-general") {
          const input: HandleContextInput = {
            message: userInput,
            conversationHistory: conversationHistory,
          };
          const result: HandleContextOutput = await handleContext(input);
          assistantResponseMessageContent = result.response;
          textForTTS = result.response;
          setConversationHistory(result.updatedConversationHistory);
          if (typeof onContentGenerated === 'function') {
            onContentGenerated(result.response, 'text', `Chat: ${playgroundTitleBase}`, null);
          }

        } else if (selectedModelValue === "together-code") {
          const detectedLanguage = userInput.toLowerCase().includes("javascript") ? "javascript" :
                                 userInput.toLowerCase().includes("python") ? "python" :
                                 userInput.toLowerCase().includes("typescript") ? "typescript" :
                                 userInput.toLowerCase().includes("html") ? "html" :
                                 userInput.toLowerCase().includes("css") ? "css" :
                                 "code"; // Default language for markdown block
          const codeInput: GenerateCodeInput = {
            taskDescription: userInput,
            programmingLanguage: detectedLanguage,
          };
          const result: GenerateCodeOutput = await generateCode(codeInput);
          
          const langForMarkdown = result.explanation ? detectedLanguage : (detectedLanguage || 'code');
          assistantResponseMessageContent = `${result.explanation ? `${result.explanation}\n\n` : ''}\`\`\`${langForMarkdown}\n${result.generatedCode}\n\`\`\``;
          textForTTS = result.explanation || `Generated ${detectedLanguage} code.`;
          
          const newHistoryEntry = `User: ${userInput}\nAssistant (${modelDisplayName}): (Code Snippet Provided)\n${result.explanation || ''}`;
          setConversationHistory(prev => `${prev}\n${newHistoryEntry}`.trim());

          if (typeof onContentGenerated === 'function') {
             onContentGenerated(result.generatedCode, 'code', `Code: ${playgroundTitleBase}`, detectedLanguage);
          }

        } else if (selectedModelValue === "together-image") {
          const imageInput: GenerateImageInput = { prompt: userInput };
          const tempImageMsgId = crypto.randomUUID();
          addMessage({ 
            id: tempImageMsgId, 
            role: "assistant", 
            content: `Generating image for: "${userInput}"...`, 
            modelUsed: modelDisplayName 
          });

          try {
            const result: GenerateImageOutput = await generateImage(imageInput);
            assistantResponseMessageContent = `An image has been generated for your prompt: "${result.promptUsed}".\n\n![Generated image for prompt: ${result.promptUsed}](${result.b64Json})`;
            textForTTS = `An image has been generated for your prompt: "${result.promptUsed}". It's now visible.`;
            setMessages(prev => prev.map(m => m.id === tempImageMsgId ? {...m, content: assistantResponseMessageContent, timestamp: new Date() } : m));
            assistantMessageId = tempImageMsgId; 
          } catch (imgError) {
             console.error("Image generation failed:", imgError);
             const errorText = imgError instanceof Error ? imgError.message : "Failed to generate image.";
             setMessages(prev => prev.map(m => m.id === tempImageMsgId ? {...m, content: `Sorry, I couldn't generate an image: ${errorText}`, timestamp: new Date() } : m));
             textForTTS = `Sorry, I couldn't generate an image. ${errorText}`;
             assistantMessageId = tempImageMsgId;
             assistantResponseMessageContent = ""; 
          }
          
          const newHistoryEntry = `User: ${userInput}\nAssistant (${modelDisplayName}): (Image generated or attempted for prompt: "${userInput}")`;
          setConversationHistory(prev => `${prev}\n${newHistoryEntry}`.trim());
          if (typeof onContentGenerated === 'function') {
            onContentGenerated("", 'image', `Image: ${playgroundTitleBase}`, null); 
          }
        } else {
           const errorMessage = "Sorry, the selected model mode is not configured correctly.";
           assistantResponseMessageContent = errorMessage;
           textForTTS = errorMessage;
        }

        if (process.env.NEXT_PUBLIC_ENABLE_TTS === 'true' && textForTTS.trim()) {
          try {
            const ttsInput: TextToSpeechInput = { text: textForTTS };
            const ttsResult: TextToSpeechOutput = await textToSpeech(ttsInput);
            if (ttsResult.audioDataUri) {
              generatedAudioDataUri = ttsResult.audioDataUri;
              playAudioFromDataUri(ttsResult.audioDataUri);
            }
          } catch (ttsError) {
            console.error("Text-to-speech generation failed:", ttsError);
            // Do not toast here, as it might be too noisy if TTS fails often
          }
        }
        
        const existingMessage = messages.find(m => m.id === assistantMessageId && m.role === 'assistant');

        if (existingMessage && selectedModelValue === 'together-image') {
           setMessages(prev => prev.map(m => 
            m.id === assistantMessageId 
            ? { ...m, audioDataUri: generatedAudioDataUri } 
            : m
          ));
        } else if (assistantResponseMessageContent) { 
           addMessage({ 
            id: assistantMessageId,
            role: "assistant", 
            content: assistantResponseMessageContent, 
            modelUsed: modelDisplayName,
            audioDataUri: generatedAudioDataUri 
          });
        }

      } catch (error) {
        console.error("AI call failed:", error);
        const errorMessageText = error instanceof Error ? error.message : "An unknown error occurred.";
        
        const imagePlaceholder = messages.find(m => m.content.startsWith("Generating image for:") && m.role === "assistant" && !m.content.includes("Sorry"));
        if (selectedModelValue === "together-image" && imagePlaceholder) {
            setMessages(prev => prev.map(m =>
                m.id === imagePlaceholder.id
                ? { ...m, content: `Sorry, I encountered an error generating the image: ${errorMessageText}` }
                : m
            ));
        } else if (!messages.find(m => m.id === assistantMessageId && m.role === 'assistant')) { 
            addMessage({ role: "assistant", content: `Sorry, I encountered an error: ${errorMessageText}`, modelUsed: "System Error"});
        }

        toast({
          title: "Error",
          description: `Failed to get response from AI: ${errorMessageText}`,
          variant: "destructive",
        });
      } finally {
        setIsLoading(false);
      }
    },
    [conversationHistory, toast, onContentGenerated, messages, playAudioFromDataUri] 
  );

  useEffect(() => {
    return () => {
      if (currentAudio.current) {
        currentAudio.current.stop();
        currentAudio.current.unload();
      }
    };
  }, []);

  return (
    <div className="flex flex-col flex-grow bg-card overflow-hidden h-full">
      <MessageList messages={messages} isLoading={isLoading} playAudio={playAudioFromDataUri} />
      <ChatInput onSendMessage={handleSendMessage} isLoading={isLoading} onNewChat={handleNewChat} />
    </div>
  );
}

